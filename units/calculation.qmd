---
title: "Calculations and Efficiency"
format:
  html:
    theme: cosmo
    css: ../assets/styles.css
    toc: true
    code-copy: true
    code-tools: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
execute:
  freeze: auto
---

```{r, chunksetup}
#| include: false
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
rm(list=ls())
library(gapminder)
```

# Working with vectors and matrices

## Vectorized calculations and comparisons

At the core of R is the idea of doing calculations on entire vectors.

```{r}
## Vectorized arithmetic
gdpTotal <- gapminder$gdpPercap * gapminder$pop

gapminder2007 <- gapminder[gapminder$year == 2007, ]

## Vectorized comparisons
wealthy <- gapminder2007$gdpPercap >= 30000

poorOrWealthy <- gapminder2007$gdpPercap >= 100000 | gapminder2007$gdpPercap < 1000
asiaWealthy <- gapminder2007$gdpPercap >= 100000 &  gapminder$continent == "Asia"

vec1 <- rnorm(5)
vec2 <- rnorm(5)
vec1 > vec2

## Vectorized boolean operations
vec1 == vec2
vec1 != vec2
## careful: 
vec1 = vec2
identical(vec1, vec2)
```

## Recycling

An important related concept is that of recycling
```{r}
vec10 <- sample(1:10, 10, replace = TRUE)
vec3 <- sample(1:10, 3, replace = TRUE)
vec5 <- sample(1:10, 5, replace = TRUE)
vec10
vec3
vec5

vec10 + vec5
vec10 + vec3
```

What choices were made by the R developers?


## Why vectorize?

Imagine how this code would look if written using a loop, or three separate loops.

```{r}
vals <- rnorm(1000)
chi2vals <- vals^2
chi2_df1000 <- sum(chi2vals)
```

Advantages:

* much faster than looping
* easier to code
* easier to read and understand the code

Vectorized code is generally fast because the underlying loop is executed in compiled C code rather than by the R interpreter (more on this [later](#whenwhy-are-loops-in-r-slow).

We've already seen that lots of functions (and operators) in R are vectorized (i.e., they can take a single value or a vector as an input argument).

## Vectorization with matrices

Recall that `+`, `-`,`*`, `/` do vectorized calculations:

```{r}
A <- matrix(1:9, 3)
B <- matrix(seq(4,36, by = 4), 3)

A*7

A + B
A + B[ , 1]
A * B
A * B[ , 1]
```

## Linear algebra: Matrix/vector multiplication

```{r}
A %*% B[ , 1]
A %*% B

identical(t(A)%*%A, crossprod(A))
```

Now let's do a bit of manipulation and see if you can infer how R represents matrices internally.

## Matrix (and array) internals

::: {.callout-tip title="Question"}

Consider our matrix 'mat':

```{r}
mat <- matrix(1:16, nrow = 4, ncol = 4)
```

Suppose I run this code: `mat[4]`.

What do you think will be returned?

1) 13
2) 4
3) 13, 14, 15, 16
4) 4, 8, 12, 16
5) an error
:::

::: {.callout-tip title="Answer" collapse="true"}

Matrices are stored column-major

```{r what_is_a_matrix, eval = FALSE}
mat[4]
attributes(mat) <- NULL
mat
is.matrix(mat)
```
This is like Fortran, MATLAB and Julia but not like C or (at least by default) Python (numpy). 
:::

## Linear algebra 

R can do essentially any linear algebra you need. It uses system-level packages called BLAS (basic linear algebra subroutines) and LAPACK (linear algebra package). Note that these calculations will be essentially as fast as if you wrote C code because R just calls C and Fortran routines to do the calculations.

The BLAS that comes with R is fairly slow. It's possible to use a [faster BLAS, as well as one that uses multiple cores automatically](https://statistics.berkeley.edu/computing/faqs/linear-algebra-and-parallelized-linear-algebra-using-blas). This can in some cases give you an order of magnitude speedup if your work involves a lot of matrix manipulations/linear algebra.


## Matrix decompositions

Here are some examples of common matrix decompositions: Cholesky decomposition, eigenvalues/eigenvectors, and SVD. These all use BLAS+LAPACK.

```{r cache=TRUE}
## Next 3 lines generate a positive definite matrix.
library(fields, warn.conflicts = FALSE, quietly = TRUE, verbose = FALSE)
times <- seq(0, 1, length = 100)
R <- exp(-rdist(times) / 0.2) # a correlation matrix
######################################################

## Some standard linear algebra operations:
e <- eigen(R)
range(e$values)
e$vectors[ , 1]

sv <- svd(R)
U <- chol(R)

devs <- rnorm(100)
Rinvb <- solve(R, devs)  # R^{-1} b
Rinv <- solve(R) # R^{-1} -- try to avoid this (slower and less numerically stable)
```

# Efficiency

## Pre-allocation

This is slow.
```{r cache=TRUE}
vals <- 0
n <- 50000
system.time({
for(i in 1:n)
      vals <- c(vals, i)
})
```

The same holds for using `rbind()`, `cbind()`, or adding to a list, one element at a time.


**Question**: Thoughts on why this is so slow? Think about what R might be doing behind the scenes in terms of storage in memory.

**Note**: This is one area where Python and some other languages handle the situation in a more sophisticated way.

## The answer is to pre-allocate memory

This is not so slow. (Please ignore the fact that this is a silly way to do this in R.)

```{r}
n <- 50000
system.time({
vals <- rep(0, n)
for(i in 1:n)
      vals[i] <- i
})
```


## apply

Some functions aren't vectorized, or you may want to use a function on every row or column of a matrix/data frame, every element of a list, etc.

For this we use the `apply()` family of functions to make our code more readable.

```{r}
mat <- matrix(rnorm(100*1000), nr = 100)
row_min <- apply(mat, MARGIN = 1, FUN = min)
col_max <- apply(mat, MARGIN = 2, FUN = max)
```

There are actually some even faster specialized functions:
```{r}
row_mean <- rowMeans(mat)
col_sum <- colSums(mat)
```

## `lapply()` and `sapply()`

These are "map" operations that apply a function to each element of a list.

```{r}
myList <- list(rnorm(3), rnorm(3), rnorm(5))
lapply(myList, min)
sapply(myList, min)
```

::: {.callout-tip title="Why use lapply and sapply?"}

The *array functions won't generally be faster than loops. Rather, they're generally used in order to have cleaner, more readable code.
:::


Note that we don't generally want to use `apply()` on a data frame. 

## `lapply()` and `sapply()` with vectors

You can use `lapply()` and `sapply()` on regular vectors, such as vectors of indices, which can come in handy. This is a bit silly but it illustrates the idea:

```{r}
myfun <- function(i) {
   max(rnorm(100))
}	

out <- lapply(1:6, myfun)
out

## Or, 'in-line' the function:

out <- sapply(1:10, function(x) x^2)
out
```

::: {.callout-tip title="Question"}

Which of these give exactly this result: `pi`, `2*pi`, `3*pi`, ...?

1) `(1:n)*pi`
2) `out <- rep(0, n); for(x in 1:n) out <- x*pi`
3) `sapply(1:n, function(x) x*pi)`
4) `out <- rep(0, n); for(x in 1:n) out[i] <- x*pi`
5) `lapply(1:n, function(x) x*pi)`
6) `sapply(1:n, "*", pi)`
7) `1:n*pi`
:::

## When/why are loops in R slow?

Consider this code:

```{r}
#| eval: false
x <- 3
x*7
x <- 'hi'
x*7
```
￼
Because of dynamic typing, when the interpreter sees `x*7` it needs to:

 - (using scoping rules) look up the value of `x`. (Consider that `x` might not even exist at the point that `x*7` is called.)
 - check if `x` is something that can be multiplied by 7 (including dealing with the fact that `x` could be a vector with many numbers in it).
 - only then can the multiplication happen.

Let’s consider writing a loop:

```{r}
#| eval: false
for(i in 1:10) {
  if(runif(1) > 0) x <- 'hi'
  if(runif(1) > 0.5) rm(x)
  x[i] <- exp(x[i])
}  
```
￼
Because of dynamic typing and lack of compilation, the interpreter needs to check if `x` exists, if it is a vector of sufficient length, if it contains numeric values, and it needs to go retrieve the required value, EVERY TIME the `exp()` is executed.

The R interpreter is a C program, so in some sense everything that happens is running as compiled code, but there are lots more things being done to accomplish a given task using interpreted code than if the task had been written directly in code that is compiled. By analogy, consider talking directly to a person in a language you both know compared to talking to a person via an interpreter who has to translate between two languages. Ultimately, the same information gets communicated (hopefully!) but the number of words spoken and time involved is much greater.


## When are loops not slow?

When the bulk of the time in the loop involves actual computation rather than checking, e.g., a loop that fits a separate machine learning model at each iteration.

Conclusions: use vectorization when you can, especially when the individual calculations are fast, but don't obsess when the individual calculations are intensive (and often will call out to C directly).

## Some efficiency tips (not R-specific)

- Consider the order of operations:
  ```{r}
  n <- 3000
  A <- matrix(rnorm(n^2),n)
  B <- matrix(rnorm(n^2),n)
  x <- rnorm(n)
  system.time(A %*% B %*% x)
  system.time(A %*% (B %*% x))
  ```
- Avoid duplicated computation
  - Don't duplicate operations within iterations of a loop unnecessarily (precompute them)
- Try to work with adjacent elements (in memory) of large vectors/matrices/arrays to [efficiently use the CPU cache](https://computing.stat.berkeley.edu/tutorial-efficient-R/efficiency#6-cache-aware-programming).
  - In R, generally work column-wise rather than row-wise with matrices
- In R, look up elements by numerical index rather than by name for O(1) computation

## Timing your code

First, a cautionary note...

> premature optimization is the root of all evil
>
> --- Donald Knuth, 1974

There are a few tools in R for timing your code.

```{r}
#| cache: true
system.time(mean(rnorm(1e7)))

library(rbenchmark)
n <- 1000
x <- matrix(rnorm(n^2), n)
benchmark(t(x) %*% x,
          crossprod(x),
          replications = 5,
          columns = c('test', 'replications', 'elapsed'))
```

Consider why the automatic crossproduct may be faster than the manual version. Note that `crossprod` calls out directly to a linear algebra system routine.

## Microbenchmark

To time code that runs very quickly, you should use the microbenchmark package. Of course one would generally only care about accurately timing quick calculations if a larger operation does the quick calculation very many times. Here’s a comparison of different ways of accessing an element of a dataframe.

```{r}
library(microbenchmark)
df <- data.frame(vals = 1:3, labs = c('a','b','c'))
vec <- c("a"=5, "b" = 7, "c" = 9)
microbenchmark(
  df[2,1],
  df$vals[2],
  df[2, 'vals'],
  vec[2],
  vec["b"]
)
```

# Memory

## Memory use

You should know how much memory (RAM) the computer you are using has and keep in mind how big your objects are and how much memory you code might use. All objects in R are stored in RAM unlike a database or certain tools for working with big data (e.g., Python's Dask package and certain R packages).

If in total, the jobs on a machine approach the physical RAM, the machine may (depending on how it is set up) start to use the hard disk as 'virtual memory'. This is called *paging* or *swapping*, and once this happens you're often toast (i.e., your code may take essentially forever to finish). And if paging doesn't happen, your job will die with an out-of-memory (OOM) error.

You can assess memory use with `top` or `ps` or `free` in Linux/Mac or the Task Manager in Windows.

Often it's a good idea to roughly estimate how much memory an object will take up even before creating it in R. You can do this with some simple arithmetic. 


```{r}
x <- matrix(rnorm(1e6*10), 1e6)
object.size(x)
1e6 * 10 *8/ 1e6  # direct calculation of Mb
print(object.size(x), units = 'auto')
```

## Garbage collection

A variable is just a name that references a location (object) in memory. When a name is used for a different object, the memory for the old object is freed to be used again.

```{r}
library(pryr)
x <- rnorm(1e8)
object.size(x)
mem_used()
x <- "hello"
mem_used()
```

## Copy-on-modify

The *semantics* of R say that `<-` creates a new copy of an object.

The *implementation* in the R interpreter  is that copies are only made when needed.

```{r}
library(pryr)
system.time(x <- rnorm(1e8))
object.size(x)
mem_used()
system.time(y <- x)
mem_used()
address(x)
address(y)

system.time(x[1] <- 3)  # Clearly more time than just modifying one element!
mem_used()
address(x)
address(y)
```

Internally R manages this by keeping track of how many variables (references) there are to a given object in memory.


## Memory and lists

[R plays various games with lists and character strings (essentially copy-on-change) to avoid redundant copies of identical data](https://stat243.berkeley.edu/stat243-fall-2022/units/unit5-programming.html#how-lists-are-stored). We won't go into details.

# Breakout

## Basics

1) Create a vector of GDP per capita in units of Euros rather than dollars.

2) Create a vector that concatenates the country and year to create a 'country-year' variable in a vectorized way using the string processing functions.

3) Use `table()` to figure out the number of countries available for each continent.

## Using the ideas

4) Explain the steps of what this code is doing: `tmp <- gapminder[ , -which(names(gapminder) == "continent")]`.

5) Compute the number of NAs in each column of the gapminder dataset using `sapply()` and making use of the `is.na()` function. It's possible to do this without writing a function.

6)  Suppose we have two categorical variables and we conduct a hypothesis test of independence. The chi-square statistic is: 

   $$
   \chi^2 = \sum_{i=1}^{n}\sum_{j=1}^{m} \frac{(y_{ij} - e_{ij})^2}{e_{ij}}, 
   $$ 

   where $e_{ij} = \frac{y_{i\cdot} y_{\cdot j}}{y_{\cdot \cdot}}$, with $y_{i\cdot}$ the sum of the values in the i'th row, $y_{\cdot j}$ the sum of values in the j'th column, and $y_{\cdot\cdot}$ the sum of all the values. Suppose I give you a matrix in R with the $y_{ij}$ values. 

   You can generate a test matrix as:
   
   ```{r, eval=FALSE}
   y <- matrix(sample(1:10, 12, replace = TRUE), 
   nrow = 3, ncol = 4)
   ```

   Compute the statistic without *any* loops as follows:

     - First, assume you have the *e* matrix. How do you compute the statistic without loops as a function of `y` and `e`?
     - How can you construct the *e* matrix? Hint: the numerator of *e* is just an *outer product* for which the `outer()` function can be used.

## Advanced 

7) Here's a cool trick using `sapply` to pull off a particular element of a list of lists:

   ```{r}
   params <- list(a = list(mn = 7, sd = 3), b = list(mn = 6,sd = 1), 
   c = list(mn = 2, sd = 1))
   
   sapply(params, "[[", 1)
   ```

   Explain what that does and why it works.

   Hint:
   ```{r}
   test <- list(5, 7, 3)
   test[[2]]
   # `[[`(test, 2)  # need it commented or R Markdown processing messes it up...

   # `+`(3, 7)
   ```
