---
title: "Parallelization and Rcpp"
format:
  html:
    theme: cosmo
    css: ../assets/styles.css
    toc: true
    code-copy: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    code-tools: true
execute:
  freeze: auto
---

# Parellelization

## Computer architecture

Note to participants: It can be troublesome to use parallelization in RStudio, so we'll just run the demo code in this module in a command line R session. You can open the basic R GUI for Mac or Windows, or, on a Mac, start R in a terminal window.

* Modern computers have multiple processors and clusters/supercomputers have multiple networked machines, each with multiple processors.
* The key to increasing computational efficiency in these contexts is breaking up the work amongst the processors.
* Processors on a single machine (or 'node') share memory and don't need to carry out explicit communication (shared memory computation)
* Processors on separate machines need to pass data across a network, often using the MPI protocol (distributed memory computation)

We'll focus on shared memory computation here.

## How do I know how many cores a computer has?

* Linux - count the processors listed in */proc/cpuinfo* or use `nproc`
* Mac - in a terminal: `system_profiler | grep -i 'Cores'`
* Windows - count the number of graphs shown for CPU Usage (or CPU Usage History) under "Task Manager->Performance", or [try this program](http://www.cpuid.com/cpuz.php) 
 
To see if multiple cores are being used by your job, you can do:

* Mac/Linux - use *top* or *ps*
* Windows - see the "Task Manager->Performance->CPU Usage"

## How can we make use of multiple cores?

Some basic approaches are:

* Use a linear algebra package that distributes computations across 'threads'
* Spread independent calculations (embarrassingly parallel problems) across multiple cores
    - *for* loops with independent calculations
    - parallelizing `lapply()` and its variants


## Threaded linear algebra

R comes with a default BLAS (basic linear algebra subroutines) and LAPACK (linear algebra package) that carry out the core linear algebra computations. However, you can generally improve performance (sometimes by an order of magnitude) by using a different BLAS. Furthermore a threaded BLAS will allow you to use multiple cores.

A 'thread' is a lightweight process, and the operating system sees multiple threads as part of a single process.

* For Linux, *openBLAS* and Intel's *MKL* are both fast and threaded. 
* For Mac, Apple's *vecLib* is fast and threaded.
* For Windows, you may be out of luck.

We'll show by demonstration that my desktop in my office is using multiple cores for linear algebra operations.

```{r}
#| cache: true
# note to CJP: don't run on laptop with slow BLAS
n <- 5000
x <- matrix(rnorm(n^2), n)
U <- chol(crossprod(x))
```

You should see that your R process is using more than 100% of CPU. Inconceivable!

## More details on the BLAS (optional)

You can talk with your systems administrator about linking R to a fast BLAS or you can look into it yourself for your personal machine; see the [R Installation and Administration manual](http://www.cran.r-project.org/manuals.html).

Note that in some cases, in particular for small matrix operations, using multiple threads may actually slow down computation, so you may want to experiment, particularly with Linux. You can force the linear algebra to use only a single core by doing (assuming you're using the bash shell) `export OMP_NUM_THREADS=1` in the terminal window *before* starting R in the same terminal. Or see the *RhpcBLASctl* package to do it from within R.
 
Finally, note that threaded BLAS and either `foreach` or parallel versions of `apply()` can conflict and cause R to hang, so you're likely to want to set the number of threads to 1 as above if you're doing explicit parallelization. 

## What is an embarrassingly parallel (EP) problem?

An EP problem is one that can be solved by doing independent computations as separate processes without communication between the processes. You can get the answer by doing separate tasks and then collecting the results. 

Examples in statistics / data science / machine learning include

1. stratified analyses
2. cross-validation
4. simulations with many independent replicates
5. bootstrapping
3. random forest models

Some things that are not EP (at least not in a basic formulation):

1. optimization
2. Markov chain Monte Carlo for fitting Bayesian models

# Using the future package

## Using multiple cores for EP problems: parallel *apply* using `future`

The `future` package provides a lot of nice features for parallelization. We'll just scratch the surface here to parallelize operations over the elements of a list (note that this is essentially equivalent to `parLapply` and `mclapply`).

First, make sure your computations on the elements are independent of each other and don't involve sequential calculations!

We'll use the airline departure dataset, with timing informatin for all flights from SFO over a period of several years. We'll do a stratified analysis, fitting a GAM (see Unit 6) for each of the destination airports.

```{r}
air <- read.csv(file.path('..', 'data', 'airline.csv'))

fitFun <- function(curDest) {
            library(mgcv)
            tmp <- subset(air, Dest == curDest)
            ## It would better to do this with date-time functionality:
            tmp$Hour <- tmp$CRSDepTime %/% 100
            
            curMod <- try(gam(DepDelay ~ Year + s(Month) + s(Hour) + 
                 as.factor(DayOfWeek), data = tmp))
            if(is(tmp, "try-error")) curMod <- NA 
            return(curMod)
}


library(future.apply)
nCores <- 4
plan(multisession, workers = nCores)  
out <- future_lapply(unique(air$Dest), fitFun)
out[[1]]
out[[81]]
```

Note that the `plan` statement determines how the parallelization is done behind the scenes. As shown here, it will start up workers locally on your computer, but if you have access to a cluster, you can modify the plan to make use of multiple compute nodes in a cluster.

One thing to keep in mind is whether the different tasks all take about the same amount of time or widely different times. In the latter case, one wants to sequentially dispatch tasks as earlier tasks finish, rather than dispatching a block of tasks to each core. See  the `future.scheduling` argument for user control over how the allocation is done. 

## Using multiple cores for EP problems: *foreach*

First, make sure your iterations are independent and don't involve sequential calculations!

The *foreach* package provides a way to do a for loop using multiple cores. It can use a variety of 'back-ends' that handle the nitty-gritty of the parallelization. Happily it integrates with the `future` package nicely. 

```{r cache=TRUE}
library(parallel)
library(doFuture)
library(foreach)

nCores <- 4  # actually only 2 on my laptop, but appears hyperthreaded
registerDoFuture()
plan(multisession, workers = nCores)  

out <- foreach(dest = unique(air$Dest)) %dopar% {
    cat("Starting job for ", dest, ".\n", sep = "")
    outSub <- fitFun(dest)
    cat("Finishing job for ", dest, ".\n", sep = "")
    outSub # this will become part of the out objec
}
out[1:2]
```

::: {.callout-tip title="Question"}

What do you think are the advantages and disadvantages of having many small tasks vs. a few large tasks?
:::

## Other "plans"

- multisession: multiple R sessions on a single machine
- multicore: multiple forked R sessions on a single machine (less copying)
- cluster: multiple R sessions on one or more machines

## Parallelization and Random Number Generation

A tale of the good, the bad, and the ugly

Random numbers on a computer are [not truly random](http://dilbert.com/strips/comic/2001-10-25) but are generated as a sequence of pseudo-random numbers. The sequence is finite (but very, very, very, very long) and eventally repeats itself. 

A random number seed determines where in the sequence one starts when generating random numbers.

* The ugly: Make sure you do not use the same seed for each task
```{r}
set.seed(1)
rnorm(5)
set.seed(1)
rnorm(5)
```
* The (not so) bad: Use a different seed for each task or each process. It's possible the subsequences will overlap but quite unlikely.

* The good: Use the L'Ecuyer algorithm to ensure distinct subsequences in each worker, which `future` makes easy:
    - with `future.apply`, use `future.seed = TRUE`.
    - with `foreach` and `%dofuture%`, include `.options.future = list(seed = TRUE)`.


# Rcpp

While R can interface with lots of other languages, the most fundamental is to interface with C/C++. In part this is because the R interpreter is a C program and under the hood, R data structures are C structs. In part this is because of the success of Rcpp as a wrapper that makes it easy to interface to C++ (including C) code.

Some comments:

 - Rcpp has a nice interface that makes it easy to work with R data structures in C++ and use various functions similar to those provided in R.
 - Loops and recursion in C++ are fast, unlike in R.
 - Rcpp allows you to make use of various libraries available in C/C++, such as the standard template library (STL), Eigen (for linear algebra), and many others.
 - Often one will write most of one's code in R and off-load the core, slow computational parts to C++, managed from R.

Some good overviews of Rcpp are:

 - [The Advanced R book by Hadley Wickham](https://adv-r.hadley.nz/rcpp.html)
 - [The Rcpp vignette](https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-introduction.pdf)
 - [Rcpp tutorial from 2019](https://dirk.eddelbuettel.com/papers/useR2019_rcpp_tutorial.pdf)


## Rcpp: compilation

Behind the scenes, the first step in using a C++ function is that the C++ code is compiled to machine (binary) code that can be called from R.

To use Rcpp, you'll need a C++ compiler:

 - for Windows: install `rtools`
 - for MacOS: install `xcode`

Here's a test that your compiler works from R:

```{r}
library("Rcpp")
evalCpp("2 + 2")
```


## Basic example

Here's a basic example of using C++ code via Rcpp from the Advanced R book.

```{r}
cppFunction('int add3(int x, int y, int z) {
  int sum = x + y + z;
  return sum;
}')

## The pause is because the C++ code has to be compiled.

add3

add3(1,3,5)
```

Note that some casting/coercing of types must be happening:

```{r}
add3(1,2,3)
add3(1L, 2L, 3L)
add3(1,2,3.7)
```

Notice we need to include the types of variables, since C++ is statically typed (in large part this is why it is much faster than R).

## A short but rich example: loop fusion

Consider this vectorized code.

```{r}
#| eval: false
x <- exp(x) + 3 * sin(x)
```

Because it's vectorized, it avoids the R overhead involved in looping, with the individual calculations (`exp`, `+`, `*`, and `sin`) done in for loops in compiled C code.

However, it has a subtle disadvantage compared to this for loop version:

```{r}
#| eval: false
n <- length(x)
for(i in 1:n)
   x[i] <- exp(x[i]) + 3 * sin(x[i])
```

::: {.callout-tip title="Question"}
What has to happen in the vectorized code that doesn't happen in the for loop?
:::

::: {.callout-tip title="Answer" collapse="true"}

- The individual operations are done as separate vectorized calls to compiled C code.
- Temporary vectors must be allocated and filled to carry out `exp`, `sin`, `*` and `+`.
  - Extra memory needed.
  - Time for allocation and moving values.
- The result `x` and the input `x` both need to exist at one time, briefly.
:::

None of that needs to happen with the loop version.

## Loop fusion using Rcpp

We can write up the for loop version using Rcpp to get the benefits of loop fusion.

```{r}
cppFunction('void fuse(NumericVector x) {
  int n = x.size();
  Rcpp::Rcout << "Working with a vector of length " << n << "." << std::endl;
  for(int i = 0; i < n; ++i) {
    x[i] = exp(x[i]) + 3 * sin(x[i]);
  }
}')

n <- 1e7
x <- rnorm(n)
y <- x

rbenchmark::benchmark(
 x <- exp(x) + 3*sin(x),
 fuse(y),
 replications = 1,  # 'x' could explode on repeated calls
 columns = c('test', 'replications', 'elapsed'))
```

We could also have created a version that doesn't overwrite the input.

```{r}
#| eval: false
cppFunction('NumericVector fuse(NumericVector x) {
  int n = x.size();
  NumericVector result(n);
  for(int i = 0; i < n; ++i) {
    result[i] = exp(x[i]) + 3 * sin(x[i]);
  }
  return result;
}')
```

## Rcpp source code files

Except for quick-and-dirty work, we'd generally want the Rcpp C++ code in a code file that can be recognized by editors as C++ code rather than in quotations in R. This will also help with debugging.

See [fuse.cpp](fuse.cpp) for the code for the version that does not overwrite the input.

```{r}
sourceCpp('fuse.cpp')

x <- rnorm(10)
y <- fuse(x)
```

## Using R classes in C++

Rcpp provides classes that correspond to most R data structures/classes. We've already seen the use of `NumericVector` for R's numeric vector.

Here's an example (from the Advanced R book) of working with a list (an S3 object) and manipulating its components in C++. We need to cast the vector components as `NumericVector`s

```{c++}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double mpe(List mod) {
  if (!mod.inherits("lm")) stop("Input must be a linear model");

  NumericVector resid = as<NumericVector>(mod["residuals"]);
  NumericVector fitted = as<NumericVector>(mod["fitted.values"]);

  int n = resid.size();
  double err = 0;
  for(int i = 0; i < n; ++i) {
    err += resid[i] / (fitted[i] + resid[i]);
  }
  return err / n;
}
```

## Rcpp "sugar"

[*Rcpp Sugar*](https://cran.r-project.org/web/packages/Rcpp/vignettes/Rcpp-sugar.pdf) provides additional functionality to make it easier to write C++ code in an R-like way.

This includes:

- Calling functions from R's C math library, e.g., `Rcpp::rnorm(10, 3, 0.5)`
- Various functions that mimic standard R functions (e.g., `all`, `is_na`)
- Using R's random number generation
- Vectorized calls (for code clarity, not for speed!)

